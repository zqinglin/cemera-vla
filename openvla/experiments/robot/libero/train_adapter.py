"""
train_adapter.py

Train the ShallowWideTransformerAdapter to map features from a "difficult" camera
view (source, e.g., eye-in-hand) to the "standard" camera view (target, e.g., agentview).

Data: paired_data generated by capture_view_pairs.py with structure:
  paired_data/
    view_A/frame_00001.png
    view_C/frame_00001.png

Core pipeline per spec:
  - Freeze OpenVLA vision encoder (no gradients)
  - Extract features for A (target) and C (source)
  - Train Adapter with MSE:  F_adapted = Adapter(F_source)  vs  F_target
  - Optimizer: AdamW (only Adapter params)
  - Save adapter_final.pth at the end

Usage example:
  python experiments/robot/libero/train_adapter.py \
    --pretrained_checkpoint hf_models/openvla-openvla-7b-finetuned-libero-spatial \
    --data_root ./paired_data \
    --epochs 3 --batch_size 8 --lr 1e-4 --out_dir ./adapter_ckpts
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Tuple

import argparse
import os
import torch
import torch.nn as nn
import torch.utils.data as data
from PIL import Image
from torchvision import transforms
from transformers import AutoProcessor, AutoImageProcessor, AutoModelForVision2Seq

from experiments.robot.libero.adapter import ShallowWideTransformerAdapter, AdapterConfig


@dataclass
class TrainConfig:
    # Models
    pretrained_checkpoint: str = "hf_models/openvla-openvla-7b-finetuned-libero-spatial"

    # Data
    data_root: str = "./paired_data"
    img_size: int = 224
    epochs: int = 3
    batch_size: int = 8
    num_workers: int = 4

    # Optim
    lr: float = 1e-4
    weight_decay: float = 1e-4

    # Misc
    device: str = "auto"  # "auto" | "cuda:0" | "cpu"
    out_dir: str = "./adapter_ckpts"
    seed: int = 7


class PairedImageDataset(data.Dataset):
    def __init__(self, root: str, transform: transforms.Compose) -> None:
        super().__init__()
        self.root = Path(root)
        self.dir_a = self.root / "view_A"
        self.dir_c = self.root / "view_C"
        self.fnames = sorted([p.name for p in self.dir_a.glob("*.png")])
        # Ensure strict pairing by name
        fnames_c = sorted([p.name for p in self.dir_c.glob("*.png")])
        assert self.fnames == fnames_c, "view_A and view_C file names must match one-to-one"
        self.transform = transform

    def __len__(self) -> int:
        return len(self.fnames)

    def __getitem__(self, idx: int) -> Tuple[Image.Image, Image.Image]:
        name = self.fnames[idx]
        img_a = Image.open(self.dir_a / name).convert("RGB")
        img_c = Image.open(self.dir_c / name).convert("RGB")
        return self.transform(img_a), self.transform(img_c)


def set_seed_all(seed: int) -> None:
    import random
    import numpy as np

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def parse_args() -> TrainConfig:
    p = argparse.ArgumentParser()
    p.add_argument("--pretrained_checkpoint", type=str, default="hf_models/openvla-openvla-7b-finetuned-libero-spatial")
    p.add_argument("--data_root", type=str, default="./paired_data")
    p.add_argument("--img_size", type=int, default=224)
    p.add_argument("--epochs", type=int, default=3)
    p.add_argument("--batch_size", type=int, default=8)
    p.add_argument("--num_workers", type=int, default=4)
    p.add_argument("--lr", type=float, default=1e-4)
    p.add_argument("--weight_decay", type=float, default=1e-4)
    p.add_argument("--device", type=str, default="auto")
    p.add_argument("--out_dir", type=str, default="./adapter_ckpts")
    p.add_argument("--seed", type=int, default=7)
    a = p.parse_args()
    return TrainConfig(
        pretrained_checkpoint=a.pretrained_checkpoint,
        data_root=a.data_root,
        img_size=a.img_size,
        epochs=a.epochs,
        batch_size=a.batch_size,
        num_workers=a.num_workers,
        lr=a.lr,
        weight_decay=a.weight_decay,
        device=a.device,
        out_dir=a.out_dir,
        seed=a.seed,
    )


def main(cfg: TrainConfig) -> None:
    set_seed_all(cfg.seed)

    # Device & dtype
    device = (
        torch.device(cfg.device)
        if cfg.device != "auto"
        else (torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu"))
    )
    dtype = torch.bfloat16 if device.type == "cuda" else torch.float32

    # Resolve processor source (fallback to HF repo if local dir lacks processor files)
    def _resolve_processor_source(ckpt: str) -> str:
        if os.path.isdir(ckpt):
            expected = [
                "processor_config.json",
                "preprocessor_config.json",
                "tokenizer_config.json",
                "vocab.json",
                "merges.txt",
            ]
            if not any(os.path.isfile(os.path.join(ckpt, f)) for f in expected):
                base = os.path.basename(os.path.abspath(ckpt)).lower()
                if "openvla-7b-finetuned-libero-spatial" in base:
                    return "openvla/openvla-7b-finetuned-libero-spatial"
                if "openvla-7b-finetuned-libero-object" in base:
                    return "openvla/openvla-7b-finetuned-libero-object"
                if "openvla-7b-finetuned-libero-goal" in base:
                    return "openvla/openvla-7b-finetuned-libero-goal"
                if "openvla-7b-finetuned-libero-10" in base:
                    return "openvla/openvla-7b-finetuned-libero-10"
                if "openvla-7b" in base:
                    return "openvla/openvla-7b"
        return ckpt

    processor_src = _resolve_processor_source(cfg.pretrained_checkpoint)
    # Load image processor (for images-only) and VLA (frozen)
    image_processor = AutoImageProcessor.from_pretrained(processor_src, trust_remote_code=True)
    vla = AutoModelForVision2Seq.from_pretrained(
        cfg.pretrained_checkpoint,
        attn_implementation="flash_attention_2",
        torch_dtype=dtype,
        low_cpu_mem_usage=True,
        trust_remote_code=True,
    ).to(device)
    vla.eval()
    for p in vla.parameters():
        p.requires_grad_(False)

    # Build adapter
    adapter_cfg = AdapterConfig(
        num_patches=256,
        token_dim=2176,
        adapter_width=2688,
        nhead=21,
        num_layers=2,
        dropout=0.0,
    )
    adapter = ShallowWideTransformerAdapter(adapter_cfg).to(device)

    # Dataset & Loader
    tfm = transforms.Compose(
        [
            transforms.Resize((cfg.img_size, cfg.img_size)),
            transforms.ToTensor(),
            transforms.ConvertImageDtype(torch.bfloat16 if dtype == torch.bfloat16 else torch.float32),
        ]
    )
    ds = PairedImageDataset(cfg.data_root, tfm)
    loader = data.DataLoader(
        ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=(device.type == "cuda")
    )

    # Optimizer & Loss
    optimizer = torch.optim.AdamW(adapter.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)
    criterion = nn.MSELoss()

    # Helper: feature extractor using frozen VLA
    def extract_feats(img_batch: torch.Tensor) -> torch.Tensor:
        # img_batch: (B,3,H,W) in [0,1]; convert using processor to match model expected preprocessing
        with torch.no_grad():
            # Build a minimal batch using processor for images only
            # processor expects PIL Image list or already batched tensors depending on custom class
            # We convert back to list of PIL for robust path
            imgs = [transforms.ToPILImage()(img.cpu().to(torch.float32)) for img in img_batch]
            proc = image_processor(images=imgs, return_tensors="pt")
            pixel_values = proc["pixel_values"].to(device, dtype=dtype)
            feat = vla.vision_backbone(pixel_values)  # (B, 256, 2176)
        return feat

    # Training loop
    Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)
    best_loss = float("inf")
    global_step = 0

    for epoch in range(cfg.epochs):
        adapter.train()
        running = 0.0
        for batch_idx, (img_a, img_c) in enumerate(loader):
            img_a = img_a.to(device)
            img_c = img_c.to(device)

            with torch.no_grad():
                f_target = extract_feats(img_a)  # from view_A
                f_source = extract_feats(img_c)  # from view_C

            pred = adapter(f_source)
            loss = criterion(pred, f_target)

            optimizer.zero_grad(set_to_none=True)
            loss.backward()
            optimizer.step()

            running += loss.item()
            global_step += 1

            if (batch_idx + 1) % 50 == 0:
                avg = running / 50.0
                print(f"epoch {epoch+1} step {batch_idx+1} loss {avg:.6f}")
                running = 0.0

        # Save latest each epoch
        latest_path = Path(cfg.out_dir) / "adapter_latest.pth"
        torch.save(adapter.state_dict(), latest_path)
        print(f"Saved latest adapter to {latest_path}")

        # Track best epoch avg (optional, here reuse last 50-window avg if available)
        if avg < best_loss:
            best_loss = avg
            best_path = Path(cfg.out_dir) / "adapter_best.pth"
            torch.save(adapter.state_dict(), best_path)
            print(f"Saved best adapter to {best_path}")

    # Final save
    final_path = Path(cfg.out_dir) / "adapter_final.pth"
    torch.save(adapter.state_dict(), final_path)
    print(f"Saved final adapter to {final_path}")


if __name__ == "__main__":
    main(parse_args())


